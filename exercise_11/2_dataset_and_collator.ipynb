{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8961dc3244f1bf",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Iterable Dataset and Dataloader\n",
    "\n",
    "In the previous exercises we have always worked with map style datasets. A map-style dataset is one that implements the __getitem__() and __len__() protocols, and represents a map from (possibly non-integral) indices/keys to data samples.\n",
    "For example, such a dataset, when accessed with dataset[idx], could read the idx-th image and its corresponding label from a folder on the disk.\n",
    "\n",
    "However, the assumption that you can trivially map to each data point in your dataset means that it is less suited to situations where the input data is arriving as part of a stream, for example, an audio or video feed. Alternatively, each datapoint might be a subset of a file which is too large to be held in memory and so requires incremental loading during training. These situations can be addressed with more complex logic in our dataset or additional pre-processing of our inputs, but there is now a more natural solution, enter the IterableDataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3079091bd79101",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## (Optional) Mount folder in Colab\n",
    "\n",
    "Uncomment the following cell to mount your gdrive if you are using the notebook in google colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "265c455ba2add7a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom google.colab import drive\\nimport os\\n\\ngdrive_path='/content/gdrive/MyDrive/i2dl/exercise_11'\\n\\n# This will mount your google drive under 'MyDrive'\\ndrive.mount('/content/gdrive', force_remount=True)\\n# In order to access the files in this notebook we have to navigate to the correct folder\\nos.chdir(gdrive_path)\\n# Check manually if all files are present\\nprint(sorted(os.listdir()))\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the following lines if you want to use Google Colab\n",
    "# We presume you created a folder \"i2dl\" within your main drive folder, and put the exercise there.\n",
    "# NOTE: terminate all other colab sessions that use GPU!\n",
    "# NOTE 2: Make sure the correct exercise folder (e.g exercise_11) is given.\n",
    "\n",
    "\"\"\"\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "gdrive_path='/content/gdrive/MyDrive/i2dl/exercise_11'\n",
    "\n",
    "# This will mount your google drive under 'MyDrive'\n",
    "drive.mount('/content/gdrive', force_remount=True)\n",
    "# In order to access the files in this notebook we have to navigate to the correct folder\n",
    "os.chdir(gdrive_path)\n",
    "# Check manually if all files are present\n",
    "print(sorted(os.listdir()))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d38fe944cedbf9d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 1. Iterable Dataset\n",
    "Let's have a look at a standard Map Style Dataset first! As always, we have to implement a __len__() method and a __getitem__ method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24551f0af6afebbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-29T16:54:42.961268Z",
     "start_time": "2023-12-29T16:54:41.405273Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, IterableDataset \n",
    "from exercise_code.data.BytePairTokenizer import load_pretrained_fast\n",
    "from exercise_code.tests.iterable_dataset_test import test_task_1\n",
    "import os\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "root_path = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "dummy_datasets_path = os.path.join(root_path, 'datasets', 'transformerDatasets', 'dummyDatasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "578df089f124b2ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-29T16:54:44.975305Z",
     "start_time": "2023-12-29T16:54:44.674714Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyMapDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79ff24a0d08ba36b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-29T16:54:44.994334Z",
     "start_time": "2023-12-29T16:54:44.975810Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3])\n",
      "tensor([4, 5, 6, 7])\n",
      "tensor([ 8,  9, 10, 11])\n"
     ]
    }
   ],
   "source": [
    "data = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
    "\n",
    "map_dataset = MyMapDataset(data)\n",
    "\n",
    "loader = DataLoader(map_dataset, batch_size=4)\n",
    "\n",
    "for batch in loader:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d68351b515912",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In comparison, let's have a look at a simple Iterable Dataset. All we need to get it running is an __iter__() method! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a64573f770b775",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-29T16:54:46.902214Z",
     "start_time": "2023-12-29T16:54:46.877787Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyIterableDateset(IterableDataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3994edf438b8053",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-29T16:54:48.646484Z",
     "start_time": "2023-12-29T16:54:47.765012Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3])\n",
      "tensor([4, 5, 6, 7])\n",
      "tensor([ 8,  9, 10, 11])\n"
     ]
    }
   ],
   "source": [
    "iter_dataset = MyIterableDateset(data)\n",
    "\n",
    "loader = DataLoader(iter_dataset, batch_size=4)\n",
    "\n",
    "for batch in loader:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332a4d00a64c44ee",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "So far not much to see, we are able to create the exact same thing as earlier with an arguably more complicated code. Let's try something more interesting: Reading from a file! With a map style dataset, we pretty much have to read the entire file into memory before we can return it in a getitem method. With our images, we only needed to store the image path in a list, however, there is no straightforward way to jump to the ith line in a text file. If we don't care so much about data shuffling, we can instead just return the next line as it comes in from the file and that is exactly what this does!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6365c4bbd02f904e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-29T16:54:48.683831Z",
     "start_time": "2023-12-29T16:54:48.666631Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyIterableDateset(IterableDataset):\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "    \n",
    "    def __len__(self):\n",
    "        return None\n",
    "    \n",
    "    def parse_file(self):\n",
    "        with open(self.file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                yield line.strip('\\n')\n",
    "                \n",
    "    def __iter__(self):\n",
    "        return self.parse_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c99bbe38b8fabbd5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-29T16:55:06.771268Z",
     "start_time": "2023-12-29T16:55:06.741866Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Iterable Datasets represent a flexible way to handle large volumes of data in machine learning pipelines.']\n",
      "['They enable the sequential processing of data, allowing for efficient handling of datasets that might not fit entirely into memory.']\n",
      "['With Iterable Datasets, you can load batches of data on-the-fly, preprocess them, and feed them into your model incrementally.']\n",
      "['This process is crucial for optimizing computational resources and handling datasets that are too large to fit into memory entirely.']\n"
     ]
    }
   ],
   "source": [
    "file_path = os.path.join(dummy_datasets_path, 'DummyFile')\n",
    "iter_dataset = MyIterableDateset(file_path=file_path)\n",
    "\n",
    "loader = DataLoader(iter_dataset, batch_size=1)\n",
    "\n",
    "for batch in loader:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f6ae5645b9705b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The nice thing is, we can even easily cycle through multiple files. This can become a very handy tool for a lot of NLP tasks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89a23069a12dcce3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-29T16:55:09.083206Z",
     "start_time": "2023-12-29T16:55:09.066296Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyIterableDateset(IterableDataset):\n",
    "    def __init__(self, file_paths):\n",
    "        self.file_paths = file_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return None\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for file_path in self.file_paths:\n",
    "            with open(file_path, 'r') as file:\n",
    "                for line in file:\n",
    "                    yield line.strip('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8c53cbf8dbf0535",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-26T10:09:04.407533Z",
     "start_time": "2023-12-26T10:09:04.389678Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dummy File 1 Line 1']\n",
      "['Dummy File 1 Line 2']\n",
      "['Dummy File 1 Line 3']\n",
      "['Dummy File 1 Line 4']\n",
      "['Dummy File 1 Line 5']\n",
      "['Dummy File 1 Line 6']\n",
      "['Dummy File 1 Line 7']\n",
      "['Dummy File 1 Line 8']\n",
      "['Last Line of File 1']\n",
      "['Dummy File 2 Line 1']\n",
      "['Dummy File 2 Line 2']\n",
      "['Dummy File 2 Line 3']\n",
      "['Dummy File 2 Line 4']\n",
      "['Dummy File 2 Line 5']\n",
      "['Last Line of File 2']\n",
      "['Dummy File 3 Line 1']\n",
      "['Dummy File 3 Line 2']\n",
      "['Dummy File 3 Line 3']\n",
      "['Dummy File 3 Line 4']\n",
      "['Dummy File 3 Line 5']\n",
      "['Dummy File 3 Line 6']\n",
      "['Dummy File 3 Line 7']\n",
      "['Dummy File 3 Line 8']\n",
      "['Dummy File 3 Line 9']\n",
      "['Dummy File 3 Line 10']\n",
      "['Dummy File 3 Line 11']\n",
      "['Last Line of File 3']\n"
     ]
    }
   ],
   "source": [
    "file_paths = [os.path.join(dummy_datasets_path, 'DummyFile1'),\n",
    "              os.path.join(dummy_datasets_path, 'DummyFile2'),\n",
    "              os.path.join(dummy_datasets_path, 'DummyFile3')]\n",
    "\n",
    "iter_dataset = MyIterableDateset(file_paths=file_paths)\n",
    "\n",
    "loader = DataLoader(iter_dataset, batch_size=1)\n",
    "\n",
    "for batch in loader:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6a8a72634762df",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 1: Implement</h3>\n",
    "    <p>Implement the <code>parse_file()</code> method in <code>exercise_code/data/TransformerDataset.py</code>. Also, check out the <code>__iter__()</code> method!\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "502ea002fcb56243",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-22T14:42:53.082980Z",
     "start_time": "2023-12-22T14:42:52.724007Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "####### Testing Started #######\n",
      "\n",
      "Test IterableDatasetKeyTest: \u001b[92mpassed!\u001b[0m\n",
      "Test IterableDatasetValueTest: \u001b[92mpassed!\u001b[0m\n",
      "\n",
      "####### Testing Finished #######\n",
      "Test TestTask1: \u001b[92mpassed!\u001b[0m -->  Tests passed: \u001b[92m2\u001b[0m/\u001b[92m2\u001b[0m\n",
      "Score: \u001b[92m100\u001b[0m/\u001b[92m100\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "_ = test_task_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a929e8619b775ffb",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 2. Collator\n",
    "So far we have usually dealt with same sized data, such as tabular data or images of a fixed resolution. In Language Processing, our inputs are often sentences, which can usually vary in length. Since we still want to be able to pass the data through the network in batches, we have to somehow make sure, that all data is of the same length. \n",
    "That is the idea behind sequence padding. To ensure that all items in a batch are of same length, we can add special tokens to the end of our individual sentences, until all sentences are as long as the longest sentence in the batch. We also have to keep track of how many padding tokens were added to the sequence, since we don't want our model to change its prediction when we add or remove pad tokens. This can be done very efficiently using padding masks.\n",
    "To implement this functionality in our Dataloader, we have to write a custom collate function. The collate function takes in a batch in form of a list and outputs the processed data in a tensor based batched format, similar to the dataloader we implemented together in exercise 03! (Remember combine_batch_dicts and batch_to_numpy?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a551a5",
   "metadata": {},
   "source": [
    "Let's have a look at what we have to do first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b24b2b04d80a18d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-29T16:59:17.196442Z",
     "start_time": "2023-12-29T16:59:17.140376Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non Padded Lengths:\n",
      "Item 0: 12\n",
      "Item 1: 10\n",
      "Item 2: 9\n",
      "Item 3: 7\n",
      "Item 4: 29\n",
      "Padded Lengths:\n",
      "Item 0: 29\n",
      "Item 1: 29\n",
      "Item 2: 29\n",
      "Item 3: 29\n",
      "Item 4: 29\n",
      "Padding Masks\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "from exercise_code.data.TransformerDataset import CustomIterableDataset\n",
    "\n",
    "batch_list = []\n",
    "batch_size = 5\n",
    "tokenizer = load_pretrained_fast()\n",
    "\n",
    "# Load the dataset we created\n",
    "file_path = os.path.join(dummy_datasets_path, 'ds_dummy')\n",
    "dataset = CustomIterableDataset(file_paths=file_path)\n",
    "iterator = iter(dataset)\n",
    "\n",
    "# Iterate through the dataset and fill it with the source sentences\n",
    "for _ in range(batch_size):\n",
    "    batch_list.append(next(iterator)['source'])\n",
    "\n",
    "# Now let's tokenize these sentences at the same time using batch encode!\n",
    "batch_input_ids = tokenizer.batch_encode_plus(batch_list)['input_ids']\n",
    "batch_padded_input_ids = tokenizer.batch_encode_plus(batch_list, padding=True)['input_ids']\n",
    "batch_masks = tokenizer.batch_encode_plus(batch_list, padding=True)['attention_mask']\n",
    "\n",
    "# Printing the results\n",
    "print('Non Padded Lengths:')\n",
    "for i, item in enumerate(batch_input_ids):\n",
    "    print('Item {0}: {1}'.format(i, len(item)))\n",
    "\n",
    "print('Padded Lengths:')\n",
    "for i, item in enumerate(batch_padded_input_ids):\n",
    "    print('Item {0}: {1}'.format(i, len(item)))\n",
    "\n",
    "print('Padding Masks')\n",
    "for item in batch_masks:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baee9f0",
   "metadata": {},
   "source": [
    "If you compare the non padded lengths to the padded lengths, you will notice that they vary quite a bit. The padded lengths are actually as long as the longest sequence in the batch! (Item 3 in our case)\n",
    "The Encoder automatically gives us the masks, we need to track which IDs were paddings and which were not! Every 0 you see stands for an added padding token!\n",
    "\n",
    "Now let's use our Collator to get the same functionality!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c60d9c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-29T16:59:19.605338Z",
     "start_time": "2023-12-29T16:59:19.584954Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['encoder_inputs', 'encoder_mask', 'decoder_inputs', 'decoder_mask', 'labels', 'label_mask', 'label_length'])\n"
     ]
    }
   ],
   "source": [
    "from exercise_code.data.TransformerCollator import CustomCollator\n",
    "\n",
    "# Define the Collator\n",
    "collator = CustomCollator(tokenizer=tokenizer)\n",
    "dataset = CustomIterableDataset(file_paths=file_path)\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=5, collate_fn=collator)\n",
    "\n",
    "# Create an iterator to iterate through the dataloader\n",
    "iterator = iter(dataloader)\n",
    "\n",
    "# Get the first batch and print the keys\n",
    "batch = next(iterator)\n",
    "print(batch.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c5e167",
   "metadata": {},
   "source": [
    "And we can of course have a look at our masks again! Note: The output is already prepared for the model, so to get it to look the same we have to do a couple minor transformations ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10d2495e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-29T16:59:21.732340Z",
     "start_time": "2023-12-29T16:59:21.705136Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding Mask\n",
      "[[1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print('Padding Mask')\n",
    "print(batch['encoder_mask'].int().squeeze(1).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9441f69d5ca027",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task 2: Check Code</h3>\n",
    "    <p>Check the <code>__call__()</code> method in <code>exercise_code/data/TransformerCollator.py</code>. We will discuss the prepare mask function and the attention mask in the next notebook!\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a414fb",
   "metadata": {},
   "source": [
    "You are now finished with this notebook and can move on to what we have came for - transformer models!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
